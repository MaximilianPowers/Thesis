from matplotlib import cm
import numpy as np
from math import exp
import pickle as pkl
import matplotlib.pyplot as plt
import argparse
import os
from torch.utils.data import DataLoader
from torch.optim import Adam
from models.data.gen_factors import SineCosineDataset
from models.unsupervised.vae.model import Encoder, Decoder, VAE
import torch.nn.functional as F
import torch
from numpy.random import seed
from torch.optim.lr_scheduler import StepLR, ExponentialLR, CosineAnnealingLR

def get_lr_scheduler(scheduler_type, optimizer, **kwargs):
    if scheduler_type == 'step':
        return StepLR(optimizer, step_size=kwargs.get('step_size', 30), gamma=kwargs.get('gamma', 0.1))
    elif scheduler_type == 'exp':
        return ExponentialLR(optimizer, gamma=kwargs.get('gamma', 0.999))
    elif scheduler_type == 'cosine':
        return CosineAnnealingLR(optimizer, T_max=kwargs.get('T_max', 50))
    else:
        return None

def get_lambda_scheduler(scheduler_type, **kwargs):
    if scheduler_type == 'ramp':
        start = kwargs.get('start', 0)
        end = kwargs.get('end', 1)
        num_epochs = kwargs.get('num_epochs', 100)
        return lambda epoch: start + 4*(end - start) * (num_epochs-epoch) / num_epochs
    elif scheduler_type == 'exp':
        start = kwargs.get('start', 0)
        end = kwargs.get('end', 1)
        num_epochs = kwargs.get('num_epochs', 100)
        return lambda epoch: start + (end - start) * exp(-epoch / num_epochs)
    elif scheduler_type == "none":
        return None
    else:
        return None

def plot_reconstructions(model, dataset, save_dir, num_samples=5):
    """
    Plot original and reconstructed waveforms.
    """
    plt.figure(figsize=(15, 6))
    
    # Sample data points from the dataset
    sampled_data = [dataset[i][0] for i in range(num_samples)]
    sampled_data = torch.stack(sampled_data)
    
    # Get the reconstructed data points
    with torch.no_grad():
        reconstructed_data, _, _ = model(sampled_data)
    
    for i in range(num_samples):
        plt.subplot(1, num_samples, i + 1)
        plt.plot(sampled_data[i].numpy(), label="Original")
        plt.plot(reconstructed_data[i].numpy(), label="Reconstructed")
        plt.title(f"Reconstruction {i+1}")
        plt.legend()
        
    plt.tight_layout()
    plt.savefig(f"{save_dir}/recon_waveforms.png")

# Adapted plot_latent_traversal function to include "true" generative factors
def plot_latent_traversal(model, save_dir, num_samples=50):
    """
    Plot waveforms generated by varying each dimension in the latent space.
    Also plots waveforms generated using the "true" generative factors for comparison.
    """
    fig, ax = plt.subplots(1, 2, figsize=(15, 6))
    viridis = cm.get_cmap('viridis', num_samples)
    for i in range(model.encoder.out_features):
        z = torch.zeros(num_samples, model.encoder.out_features)
        z[:, i] = torch.linspace(-2, 2, num_samples)
        
        # Generate waveforms by varying the i-th latent dimension
        with torch.no_grad():
            generated_data = model.decoder(z)
        
        for j in range(num_samples):
            ax[i].plot(generated_data[j].numpy(), label=f"Latent Dim {j+1}", color=viridis(j / num_samples))
        plt.colorbar(cm.ScalarMappable(cmap=viridis), label='Traversal Value', ax=ax[i])
        plt.title(f"Latent Dim {i+1}", fontsize=16)
        
    plt.tight_layout()
    plt.savefig(f"{save_dir}/latent_traversal_waveforms_with_true.png")

def calculate_avg_gradients(model):
    avg_grads = []
    for param in model.parameters():
        if param.grad is not None:
            avg_grads.append(torch.mean(torch.abs(param.grad)).item())
    return sum(avg_grads)/len(avg_grads)

# Define the VAE loss function (negative ELBO)
def vae_loss(x_reconstructed, x, mu, log_var):
    # Reconstruction loss (MSE)
    recon_loss = F.mse_loss(x_reconstructed, x, reduction='sum')
    # KL divergence loss
    kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
    
    return recon_loss, kl_loss

def train(model, loader, criterion, optimizer, lambda_, C=0):

    torch.manual_seed(args.seed)
    model.train()
    epoch_loss = 0
    store_loss = []
    store_recon_loss = []
    store_kl_loss = []
    avg_grads = []
    for x, _ in loader:
        # Forward pass
        x_reconstructed, mu, log_var = model(x)

        #if lambda_ >= 0.5:
        #    plt.scatter(x_reconstructed.detach().numpy()[:, 0], x_reconstructed.detach().numpy()[:, 1], c='r', label='Reconstructed')
        #    plt.scatter(norm_x.detach().numpy()[:, 0], norm_x.detach().numpy()[:, 1], c='b', label='Original')
        #    plt.legend()
        #    plt.show()
        #    assert False
        #x_reconstructed = x_reconstructed * norm

        recon_loss, kl_loss = criterion(x_reconstructed, x, mu, log_var)
        loss = recon_loss + lambda_*(kl_loss-C)

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        epoch_loss += loss.item()
        store_loss.append(loss.item())
        store_recon_loss.append(recon_loss.item())
        store_kl_loss.append(lambda_*kl_loss.item())
        avg_grads.append(calculate_avg_gradients(model))
    return epoch_loss / len(loader.dataset), sum(store_loss)/len(loader.dataset), \
        sum(store_recon_loss)/len(loader.dataset), sum(store_kl_loss)/len(loader.dataset), \
        sum(avg_grads)/len(loader.dataset)

def main(args):
    torch.manual_seed(args.seed)
    seed(args.seed)
    if args.name != "none":
        model_name = args.name
    else:
        model_name = f"L{args.lambda_}_C{args.C}_lr{args.learning_rate}_lam_s{args.lambda_scheduler}_lr_s{args.lr_scheduler}"
    save_dir = f"models/unsupervised/vae/saved_models/{model_name}"
    plot_dir = f"models/unsupervised/vae/figures/{model_name}"
    os.makedirs(save_dir, exist_ok=True)
    os.makedirs(plot_dir, exist_ok=True)

    # Initialize the VAE model
    features = args.nn_shape
    encoder = Encoder(in_features=args.in_dim, features=features, out_features=args.out_dim)
    decoder = Decoder(in_features=args.out_dim, features=list(reversed(features)), out_features=args.in_dim)
    vae = VAE(encoder, decoder)
    # Create the dataset and dataloader
    if args.dataset == "sine_cosine":
        dataset = SineCosineDataset(args.n_samples, args.in_dim, random_seed=args.seed)

    
    with open(f"{save_dir}/dataset.pkl", "wb") as f:
        pkl.dump(dataset, f)
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)
    # Initialize the optimizer
    optimizer = Adam(vae.parameters(), lr=args.learning_rate)
    # Training loop
    num_epochs = args.num_epochs
    store_loss = []
    store_recon_loss = []
    store_kl_loss = []
    all_avg_grads = []
    store_kl_loss_ = 0
    lambda_scheduler = get_lambda_scheduler(args.lambda_scheduler, num_epochs=num_epochs)
    lr_scheduler = get_lr_scheduler(args.lr_scheduler, optimizer)
    for epoch in range(num_epochs):
        #lambda_ = lambda_scheduler(epoch) if lambda_scheduler else args.lambda_
        if epoch < 1000:
            lambda_ = 0.1
        elif epoch < 1400:
            lambda_ = 0.1 + (epoch-1000)/400
        elif epoch < 1800:
            lambda_ = 1.1

        C = args.C
        epoch_loss, store_loss_, store_recon_loss_, store_kl_loss_, grads_ = train(vae, dataloader, vae_loss, optimizer, lambda_=lambda_, C=C)    
        store_loss.append(store_loss_)
        store_recon_loss.append(store_recon_loss_)
        store_kl_loss.append(store_kl_loss_)
        all_avg_grads.append(grads_)
        if epoch % args.SAVE_LOG == 0:
            torch.save(vae.state_dict(), os.path.join(save_dir, f"model_{epoch}.pth"))
            if args.verbose:
                print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Lamda: {lambda_:.4f}, Grad: {grads_:.4f}, Learning rate: {optimizer.param_groups[0]['lr']:.4f}")
        if lr_scheduler:
            lr_scheduler.step()
    if args.PLOT_LOSS:
        plt.plot(store_loss[10:])
        plt.plot(store_recon_loss[10:])
        plt.plot(store_kl_loss[10:])
        plt.legend(["Total loss", "Reconstruction loss", "KL loss"])

        plt.savefig(f"{plot_dir}/loss.png")
        plt.close()
        plt.plot(all_avg_grads)
        plt.savefig(f"{plot_dir}/avg_grads.png")
        plt.close()

    if args.PLOT_MODEL:
        plot_reconstructions(vae, dataset, plot_dir, args.n_plot)
        plot_latent_traversal(vae, plot_dir)




if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--num_epochs', type=int, default=400)
    parser.add_argument('--batch_size', type=int, default=128)
    parser.add_argument('--learning_rate', type=float, default=0.002)

    parser.add_argument('--out_dim', type=int, default=2)
    parser.add_argument('--in_dim', type=int, default=32)
    parser.add_argument('--nn_shape', type=int, nargs='+', default=[64, 32, 16, 8 ,4], help='Shape of the neural network layers')

    parser.add_argument('--n_samples', type=int, default=3000)
    parser.add_argument('--dataset', type=str, default='sine_cosine')

    parser.add_argument('--lambda_', type=float, default=0.1)
    parser.add_argument('--C', type=float, default=0)
    parser.add_argument('--lambda_scheduler', type=str, default='step')
    parser.add_argument('--lr_scheduler', type=str, default='exp')

    parser.add_argument('--name', type=str, default="disentanglement_4")
    parser.add_argument('--n_plot', type=int, default=5)
    parser.add_argument('--seed', type=int, default=2)
    parser.add_argument('--PLOT_LOSS', type=bool, default=True)
    parser.add_argument('--PLOT_MODEL', type=bool, default=True)
    parser.add_argument('--SAVE_LOG', type=int, default=100)
    parser.add_argument('--verbose', type=bool, default=True)
    args = parser.parse_args()
    main(args)

