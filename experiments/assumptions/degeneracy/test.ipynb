{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('../../../')\n",
    "\n",
    "from experiments.assumptions.degeneracy.script import eigenvalue_result, eigenvalue_results_large, plot_rank_train, rank_over_training\n",
    "from models.supervised.mlp.model import MLP\n",
    "from models.supervised.bimt.model import BioMLP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def degeneracy_plot(g, surface, activations_np, labels, save_path, wrt=\"layer_wise\", precision=7):\n",
    "    layers = len(g)\n",
    "    fig, ax = plt.subplots(1, layers-1, figsize=(10*(layers-1), 10))\n",
    "\n",
    "    for layer in range(layers-1):\n",
    "        surface_ = surface[layer]\n",
    "        activations_np_ = activations_np[layer]\n",
    "        g_ = g[layer]\n",
    "        eigenval = np.linalg.eigvals(g_)\n",
    "        eigenval = np.round(eigenval, precision)\n",
    "        cnt_zero_eigenval = np.sum(np.abs(eigenval) > 1e-5, axis=1)\n",
    "\n",
    "        color = ax[layer].scatter(surface_[:, 0], surface_[:, 1], c=cnt_zero_eigenval, vmin=cnt_zero_eigenval.min(), vmax=cnt_zero_eigenval.max(), s=10, cmap=\"viridis\")\n",
    "        ax[layer].scatter(activations_np_[:, 0], activations_np_[:, 1], c=labels, s=10, alpha=0.5, cmap=\"RdBu_r\")\n",
    "        ax[layer].set_title(f\"Rank of Metric Tensor over the Manifold - Layer {layer+1}\")\n",
    "        plt.colorbar(color, ax=ax[layer])\n",
    "        ax[layer].set_xlabel(\"x\")\n",
    "        ax[layer].set_ylabel(\"y\")\n",
    "    path = f\"{save_path}\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    fig.savefig(f\"{path}/_{wrt}_degeneracy.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_eigenvalue_spectra(g, surface, activations_np, labels, save_path, wrt=\"layer_wise\", precision=7):\n",
    "\n",
    "    layers = len(g)\n",
    "    fig, ax = plt.subplots(2, layers-1, figsize=(10*(layers-1), 20))\n",
    "\n",
    "    for layer in range(layers-1):\n",
    "        surface_ = surface[layer]\n",
    "        activations_np_ = activations_np[layer]\n",
    "        g_ = g[layer]\n",
    "        eigenval = np.linalg.eigvals(g_)\n",
    "        eigenval = np.round(eigenval, precision).real\n",
    "        min_eigenvalues = np.min(eigenval, axis=1)\n",
    "        max_eigenvalues = np.max(eigenval, axis=1)\n",
    "\n",
    "\n",
    "        color = ax[0][layer].scatter(surface_[:, 0], surface_[:, 1], c=min_eigenvalues, vmin=min_eigenvalues.min(), vmax=min_eigenvalues.max(), s=10, cmap=\"viridis\")\n",
    "        ax[0][layer].scatter(activations_np_[:, 0], activations_np_[:, 1], c=labels, s=10, alpha=0.5, cmap=\"RdBu_r\")\n",
    "        ax[0][layer].set_title(f\"Minimum Eigenvalue - Layer {layer+1}\")\n",
    "        plt.colorbar(color, ax=ax[0][layer])\n",
    "        ax[0][layer].set_xlabel(\"x\")\n",
    "        ax[0][layer].set_ylabel(\"y\")\n",
    "\n",
    "        color = ax[1][layer].scatter(surface_[:, 0], surface_[:, 1], c=min_eigenvalues, vmin=max_eigenvalues.min(), vmax=max_eigenvalues.max(), s=10, cmap=\"viridis\")\n",
    "        ax[1][layer].scatter(activations_np_[:, 0], activations_np_[:, 1], c=labels, s=10, alpha=0.5, cmap=\"RdBu_r\")\n",
    "        ax[1][layer].set_title(f\"Maximum Eigenvalue - Layer {layer+1}\")\n",
    "        plt.colorbar(color, ax=ax[1][layer])\n",
    "        ax[1][layer].set_xlabel(\"x\")\n",
    "        ax[1][layer].set_ylabel(\"y\")\n",
    "\n",
    "    path = f\"{save_path}\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    fig.savefig(f\"{path}/_{wrt}_eigenvalue_spectra.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def pseudo_riemann(g, surface, activations_np, labels, save_path, wrt=\"layer_wise\", precision=7):\n",
    "    layers = len(g)\n",
    "    fig, ax = plt.subplots(1, layers-1, figsize=(10*(layers-1), 10))\n",
    "\n",
    "    for layer in range(layers-1):\n",
    "        surface_ = surface[layer]\n",
    "        activations_np_ = activations_np[layer]\n",
    "        g_ = g[layer]\n",
    "        eigenval = np.linalg.eigvals(g_)\n",
    "        eigenval = np.round(eigenval, precision)\n",
    "        neg_eigenvalues = np.sum(eigenval < 0, axis=1)\n",
    "        color = ax[layer].scatter(surface_[:, 0], surface_[:, 1], c=neg_eigenvalues, vmin=neg_eigenvalues.min(), vmax=neg_eigenvalues.max(), s=10, cmap=\"viridis\")\n",
    "        ax[layer].scatter(activations_np_[:, 0], activations_np_[:, 1], c=labels, s=10, alpha=0.5, cmap=\"RdBu_r\")\n",
    "        ax[layer].set_title(f\"Number of Negative Eigenvalues over the Manifold - Layer {layer+1}\")\n",
    "        plt.colorbar(color, ax=ax[layer])\n",
    "        ax[layer].set_xlabel(\"x\")\n",
    "        ax[layer].set_ylabel(\"y\")\n",
    "    path = f\"{save_path}\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    fig.savefig(f\"{path}/_{wrt}_pseudo_riemann.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def pseudo_riemann_det(g, surface, activations_np, labels, save_path, wrt=\"layer_wise\", precision=7):\n",
    "    layers = len(g)\n",
    "    fig, ax = plt.subplots(1, layers-1, figsize=(10*(layers-1), 10))\n",
    "\n",
    "    for layer in range(layers-1):\n",
    "        surface_ = surface[layer]\n",
    "        activations_np_ = activations_np[layer]\n",
    "        g_ = g[layer]\n",
    "        det_g = np.linalg.det(g_)\n",
    "        det_g = np.round(det_g, precision)\n",
    "\n",
    "        color = ax[layer].scatter(surface_[:, 0], surface_[:, 1], c=det_g, vmin=det_g.min(), vmax=det_g.max(), s=10, cmap=\"viridis\")\n",
    "        ax[layer].scatter(activations_np_[:, 0], activations_np_[:, 1], c=labels, s=10, alpha=0.5, cmap=\"RdBu_r\")\n",
    "        ax[layer].set_title(f\"Determinant of $g$ over the Manifold - Layer {layer+1}\")\n",
    "        plt.colorbar(color, ax=ax[layer])\n",
    "        ax[layer].set_xlabel(\"x\")\n",
    "        ax[layer].set_ylabel(\"y\")\n",
    "    path = f\"{save_path}\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    fig.savefig(f\"{path}/_{wrt}_det_pseudo_riemann.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def eigenvalue_distribution(g, save_path, wrt=\"layer_wise\", precision=7):\n",
    "    get_max_rank = 0\n",
    "    for layer_g in g[:-1]:\n",
    "        get_max_rank = max(get_max_rank, max(np.linalg.matrix_rank(layer_g)))\n",
    "    max_dim = get_max_rank\n",
    "    fig, ax = plt.subplots(max_dim, len(g)-1, figsize=(10*(len(g)-1), 10*max_dim))\n",
    "    for layer, layer_g in enumerate(g[:-1]):\n",
    "        eigenvals = np.linalg.eigvals(layer_g).real\n",
    "        eigenvals = np.round(eigenvals, precision)\n",
    "        eigenvals = np.abs(eigenvals)*np.log(np.abs(eigenvals)+1)\n",
    "        sorted_eigenvals = np.sort(eigenvals, axis=1)\n",
    "        if sorted_eigenvals.shape[-1] > max_dim: \n",
    "            sorted_eigenvals = sorted_eigenvals[:, -max_dim:]\n",
    "        \n",
    "        for indx, eig in enumerate(sorted_eigenvals.T):\n",
    "            ax[indx][layer].hist(eig)\n",
    "            ax[indx][layer].set_title(f\"Log Eigenvalue {indx+1} - Layer {layer}\")\n",
    "    path = f\"{save_path}\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    fig.savefig(f\"{path}/_{wrt}_eigenvalue_distribution.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_rank(g, save_path, wrt=\"layer_wise\", precision=7):\n",
    "    res = []        \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(max(2*(len(g)-1), 10), 8))\n",
    "    q_25, med, q_75 = [], [], []\n",
    "    for layer_g in g[:-1]:\n",
    "        eigenvalues = np.linalg.eigvals(layer_g).real\n",
    "        eigenvalues = np.round(eigenvalues, precision)\n",
    "        ranks = np.sum(np.abs(eigenvalues) > 1e-5, axis=1)/np.shape(eigenvalues)[-1]\n",
    "        res.append(ranks)\n",
    "        q_25.append(np.quantile(ranks, 0.25))\n",
    "        med.append(np.quantile(ranks, 0.5))\n",
    "        q_75.append(np.quantile(ranks, 0.75))\n",
    "\n",
    "    ax.violinplot(res, showmedians=True)\n",
    "    ax.set_xlabel('Layer Index')\n",
    "    ax.set_ylabel('Metric Rank')\n",
    "    ax.set_title('Distribution of Metric Rank over Layers with Medians')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis='y')    \n",
    "    path = f\"{save_path}\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    fig.savefig(f\"{path}/_{wrt}_rank_distribution.png\")\n",
    "    plt.close(fig)\n",
    "    return q_25, med, q_75\n",
    "\n",
    "def eigenvalue_result(input_, model, N, labels, save_path, wrt=\"layer_wise\", sigma=0.05, precision=7):\n",
    "    X = torch.from_numpy(input_).float()\n",
    "    model.forward(X, save_activations=True)\n",
    "\n",
    "    activations = model.get_activations()\n",
    "    activations_np = [a.detach().numpy() for a in activations]\n",
    "\n",
    "    g, surface = pullback_metric(model, activations, N, wrt=wrt, method=\"manifold\", sigma=sigma, normalised=True)\n",
    "    eigenvalue_distribution(g, wrt=wrt, precision=precision, save_path=save_path)\n",
    "    plot_rank(g, wrt=wrt, precision=precision, save_path=save_path)\n",
    "    degeneracy_plot(g, surface, activations_np, labels, wrt=wrt, save_path=save_path, precision=precision)    \n",
    "    plot_eigenvalue_spectra(g, surface, activations_np, labels, wrt=wrt, save_path=save_path, precision=precision)\n",
    "    pseudo_riemann(g, surface, activations_np, labels, wrt=wrt, save_path=save_path, precision=precision)\n",
    "    pseudo_riemann_det(g, surface, activations_np, labels, wrt=wrt, save_path=save_path, precision=precision)\n",
    "\n",
    "def plot_rank_train(q_25, med, q_75, savepath, wrt=\"layer_wise\"):\n",
    "    fig, ax = plt.subplots(1, len(q_25[0]), figsize=(10*len(q_25[0]), 5))\n",
    "    q_25, med, q_75 = np.array(q_25), np.array(med), np.array(q_75)\n",
    "    for indx in range(len(q_25[0])):\n",
    "        ax[indx].plot(med[:, indx], label=\"Median\")\n",
    "        ax[indx].fill_between(list(range(len(q_25))), q_25[:, indx], q_75[:, indx], alpha=0.2)\n",
    "        ax[indx].set_xlabel('Epoch')\n",
    "        ax[indx].set_ylabel('Metric Rank')\n",
    "        ax[indx].set_title('Normalised Metric Rank throughout Training')\n",
    "        ax[indx].legend()\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    path = f\"{savepath}\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    fig.savefig(f\"{path}/_{wrt}_rank_evolution.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def eigenvalue_results_large(input_, model, N, save_path, wrt=\"layer_wise\", sigma=0.05, precision=7, sampling=\"manifold\"):\n",
    "    print('checkpoint 1')\n",
    "    X = torch.from_numpy(input_).float()\n",
    "    model.forward(X, save_activations=True)\n",
    "    print(\"Gone forward\")\n",
    "    activations = model.get_activations()\n",
    "    g, _ = pullback_metric(model, activations, N, wrt=wrt, method=sampling, sigma=sigma, normalised=False)\n",
    "    print(\"Gone back\")\n",
    "    eigenvalue_distribution(g, wrt=wrt, precision=precision, save_path=save_path)\n",
    "    q_25, med, q_75 = plot_rank(g, wrt=wrt, precision=precision, save_path=save_path)\n",
    "    return q_25, med, q_75\n",
    "\n",
    "\n",
    "def rank_over_training(input_, model, N, wrt=\"layer_wise\", sigma=0.05, precision=7):\n",
    "    X = torch.from_numpy(input_).float()\n",
    "    model.forward(X, save_activations=True)\n",
    "\n",
    "    activations = model.get_activations()\n",
    "    g, _ = pullback_metric(model, activations, N, wrt=wrt, method=\"manifold\", sigma=sigma, normalised=False)\n",
    "  \n",
    "    q_25, med, q_75 = [], [], []\n",
    "    for layer_g in g[:-1]:\n",
    "        eigenvalues = np.linalg.eigvals(layer_g).real\n",
    "        eigenvalues = np.round(eigenvalues, precision)\n",
    "        ranks = np.sum(np.abs(eigenvalues) > 1e-5, axis=1)/np.shape(eigenvalues)[-1]\n",
    "        q_25.append(np.quantile(ranks, 0.25))\n",
    "        med.append(np.quantile(ranks, 0.5))\n",
    "        q_75.append(np.quantile(ranks, 0.75))\n",
    "\n",
    "    return q_25, med, q_75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from riemannian_geometry.computations.riemann_metric import LocalDiagPCA\n",
    "from utils.plotting.mesh import generate_lattice\n",
    "from utils.metrics.metrics import z_normalise\n",
    "from riemannian_geometry.computations.sample import generate_manifold_sample, sample_points_heat_kernel\n",
    "from riemannian_geometry.differential_geometry.curvature import batch_curvature, batch_vectorised_christoffel_symbols\n",
    "import torch\n",
    "from torch.func import vmap, jacfwd, jacrev\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from scipy.sparse.linalg import eigsh\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import ghalton\n",
    "from scipy.spatial import KDTree\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "def generate_halton_points(point_dataset, N):\n",
    "    # Calculate the dimensionality of the dataset\n",
    "    dim = point_dataset.shape[1]\n",
    "    \n",
    "    # Initialize the Halton sequence generator\n",
    "    sequencer = ghalton.Halton(dim)\n",
    "    \n",
    "    # Generate N points\n",
    "    halton_points = np.array(sequencer.get(N))\n",
    "    \n",
    "    # Scale the Halton points to match the range of the original dataset\n",
    "    max_ = np.max(point_dataset, axis=0)+1e-2\n",
    "    min_ = np.min(point_dataset, axis=0)-1e-2\n",
    "\n",
    "    scaled_halton_points = halton_points * (max_ - min_) + min_\n",
    "    \n",
    "    return scaled_halton_points\n",
    "\n",
    "def rejection_sampling(manifold, sampled_points, tol=1e-4):\n",
    "    mask = manifold.metric_tensor(sampled_points.transpose(), nargout=1)\n",
    "    mask = np.prod(1/np.diagonal(mask, axis1=1, axis2=2), axis=1)  > tol\n",
    "    return sampled_points[mask]\n",
    "\n",
    "def generate_manifold_sample(manifold, activations, N, tol=None):\n",
    "    if tol is None:\n",
    "        tol = manifold.rho**2\n",
    "    halton_points = generate_halton_points(activations, N)\n",
    "    return rejection_sampling(manifold, halton_points, tol=tol)\n",
    "\n",
    "def find_k_approximate_neighbors(annoy, query_vector, k=5):\n",
    "    return annoy.get_nns_by_vector(query_vector, k)\n",
    "\n",
    "def construct_graph(points, k_neighbors=5):\n",
    "    \"\"\"\n",
    "    Construct a k-nearest neighbor graph from the given points.\n",
    "    Returns the weighted adjacency matrix W and degree matrix D.\n",
    "    \"\"\"\n",
    "    if len(points.shape) > 2:\n",
    "        prod = np.prod(points.shape[1:])\n",
    "        points = points.reshape(points.shape[0], prod)\n",
    "        G = nx.Graph()\n",
    "        dim=points.shape[-1]\n",
    "\n",
    "        annoy = AnnoyIndex(dim, metric='euclidean')\n",
    "        for i, vector in enumerate(points):\n",
    "            annoy.add_item(i, vector)\n",
    "        annoy.build(int(np.sqrt(dim)))\n",
    "        for indx, point in enumerate(points):\n",
    "            result = find_k_approximate_neighbors(annoy, point, k=k_neighbors+1)[1:]\n",
    "            for neighbor in result:\n",
    "                G.add_edge(indx, neighbor)\n",
    "        W = nx.adjacency_matrix(G).todense()\n",
    "        D = np.diag(np.sum(W, axis=1))\n",
    "        return W, D\n",
    "    else:\n",
    "        tree = KDTree(points)\n",
    "        _, indices = tree.query(points, k=k_neighbors+1)  # +1 to include the point itself\n",
    "\n",
    "        n = len(points)\n",
    "        W = np.zeros((n, n))\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in indices[i]:\n",
    "                if i != j:\n",
    "                    W[i, j] = np.exp(-np.linalg.norm(points[i] - points[j])**2)\n",
    "                    W[j, i] = W[i, j]\n",
    "\n",
    "        D = np.diag(np.sum(W, axis=1))\n",
    "        return W, D\n",
    "\n",
    "def find_minimal_connected_graph(points, start_k=None, connected_components=2):\n",
    "    if start_k == None:\n",
    "        start_k = int(np.sqrt(len(points)))\n",
    "    W_start, D_start = construct_graph(points, k_neighbors=start_k)\n",
    "    G_start = nx.from_numpy_array(W_start)\n",
    "    if nx.number_connected_components(G_start) <= connected_components:\n",
    "        W, D = W_start, D_start\n",
    "        for k in reversed(range(1, start_k)):\n",
    "            W_tmp, D_tmp = construct_graph(points, k_neighbors=k)\n",
    "            G = nx.from_numpy_array(W)\n",
    "            if nx.number_connected_components(G) > connected_components:\n",
    "                return W, D, k+1\n",
    "            else:\n",
    "                W, D = W_tmp, D_tmp\n",
    "    if nx.number_connected_components(G_start) > connected_components:\n",
    "        k_max = len(points)//2\n",
    "        W, D = W_start, D_start\n",
    "        for k in range(start_k+1, k_max):\n",
    "            W_tmp, D_tmp = construct_graph(points, k_neighbors=k)\n",
    "            G = nx.from_numpy_array(W)\n",
    "            if nx.number_connected_components(G) <= connected_components:\n",
    "                return W, D, k\n",
    "            else:\n",
    "                W, D = W_tmp, D_tmp\n",
    "    return W_start, D_start, start_k    \n",
    "\n",
    "def compute_heat_kernel(W, D, t, dim):\n",
    "    \"\"\"Compute the heat kernel for a given time t using the adjacency matrix W and degree matrix D.\"\"\"\n",
    "    L = np.linalg.inv(np.sqrt(D)).dot(D - W).dot(np.linalg.inv(np.sqrt(D)))\n",
    "    lambdas, phis = eigsh(L, k=dim, which='SM', tol=1e-5)  # Compute the 10 smallest eigenvalues and eigenvectors\n",
    "\n",
    "    K_t = np.zeros_like(W, dtype=np.float64)\n",
    "    for i in range(len(lambdas)):\n",
    "        K_t += np.exp(-lambdas[i].real * t) * np.outer(phis[:, i].real, phis[:, i].real)\n",
    "\n",
    "    return K_t\n",
    "\n",
    "def sample_using_heat_kernel(points, K_t, num_samples=10):\n",
    "    \"\"\"Sample new points using the heat kernel.\"\"\"\n",
    "    n = len(points)\n",
    "    new_points = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # Pick a random starting point\n",
    "        i = np.random.randint(n)\n",
    "        \n",
    "        # Sample a new point based on the heat distribution from point i\n",
    "        j = np.random.choice(n, p=K_t[i]**2 / np.sum(K_t[i]**2))\n",
    "        \n",
    "        # Take the midpoint between the two points as the new point\n",
    "        new_points.append((points[i] + points[j]) / 2)\n",
    "\n",
    "    return np.array(new_points)\n",
    "\n",
    "def sample_points_heat_kernel(points, num_samples=10, t=0.1, connect_components=1):\n",
    "    \"\"\"Sample new points using the heat kernel.\"\"\"\n",
    "    flag = len(points.shape) > 2\n",
    "    W, D, k = find_minimal_connected_graph(points, connected_components=connect_components, start_k=None)\n",
    "    print(f\"Using {k} nearest neighbors\")\n",
    "    if flag:\n",
    "        re_points = points.reshape(points.shape[0], np.prod(points.shape[1:]))\n",
    "    K_t = compute_heat_kernel(W, D, t, dim=re_points.shape[1])\n",
    "    new_points = sample_using_heat_kernel(re_points, K_t, num_samples=num_samples)\n",
    "    if flag:\n",
    "        new_points = new_points.reshape(num_samples, points.shape[-2], points.shape[-1])\n",
    "    return new_points\n",
    "\n",
    "def uniform_sample(n_samples, dataset):\n",
    "    max_ = np.max(dataset, axis=0) + np.random.uniform(0, 0.1, size=dataset.shape[1])\n",
    "    min_ = np.min(dataset, axis=0) + np.random.uniform(0, 0.1, size=dataset.shape[1])\n",
    "    return np.random.uniform(min_, max_, size=(n_samples, dataset.shape[1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from riemannian_geometry.computations.riemann_metric import LocalDiagPCA\n",
    "from utils.plotting.mesh import generate_lattice\n",
    "from utils.metrics.metrics import z_normalise\n",
    "from riemannian_geometry.computations.sample import generate_manifold_sample, sample_points_heat_kernel\n",
    "from riemannian_geometry.differential_geometry.curvature import batch_curvature, batch_vectorised_christoffel_symbols\n",
    "import torch\n",
    "from torch.func import vmap, jacfwd, jacrev\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from scipy.sparse.linalg import eigsh\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import ghalton\n",
    "from scipy.spatial import KDTree\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "def generate_halton_points(point_dataset, N):\n",
    "    # Calculate the dimensionality of the dataset\n",
    "    dim = point_dataset.shape[1]\n",
    "    \n",
    "    # Initialize the Halton sequence generator\n",
    "    sequencer = ghalton.Halton(dim)\n",
    "    \n",
    "    # Generate N points\n",
    "    halton_points = np.array(sequencer.get(N))\n",
    "    \n",
    "    # Scale the Halton points to match the range of the original dataset\n",
    "    max_ = np.max(point_dataset, axis=0)+1e-2\n",
    "    min_ = np.min(point_dataset, axis=0)-1e-2\n",
    "\n",
    "    scaled_halton_points = halton_points * (max_ - min_) + min_\n",
    "    \n",
    "    return scaled_halton_points\n",
    "\n",
    "def rejection_sampling(manifold, sampled_points, tol=1e-4):\n",
    "    mask = manifold.metric_tensor(sampled_points.transpose(), nargout=1)\n",
    "    mask = np.prod(1/np.diagonal(mask, axis1=1, axis2=2), axis=1)  > tol\n",
    "    return sampled_points[mask]\n",
    "\n",
    "def generate_manifold_sample(manifold, activations, N, tol=None):\n",
    "    if tol is None:\n",
    "        tol = manifold.rho**2\n",
    "    halton_points = generate_halton_points(activations, N)\n",
    "    return rejection_sampling(manifold, halton_points, tol=tol)\n",
    "\n",
    "def find_k_approximate_neighbors(annoy, query_vector, k=5):\n",
    "    return annoy.get_nns_by_vector(query_vector, k)\n",
    "\n",
    "def construct_graph(points, k_neighbors=5):\n",
    "    \"\"\"\n",
    "    Construct a k-nearest neighbor graph from the given points.\n",
    "    Returns the weighted adjacency matrix W and degree matrix D.\n",
    "    \"\"\"\n",
    "    if len(points.shape) > 2:\n",
    "        prod = np.prod(points.shape[1:])\n",
    "        points = points.reshape(points.shape[0], prod)\n",
    "        G = nx.Graph()\n",
    "        dim=points.shape[-1]\n",
    "\n",
    "        annoy = AnnoyIndex(dim, metric='euclidean')\n",
    "        for i, vector in enumerate(points):\n",
    "            annoy.add_item(i, vector)\n",
    "        annoy.build(int(np.sqrt(dim)))\n",
    "        for indx, point in enumerate(points):\n",
    "            result = find_k_approximate_neighbors(annoy, point, k=k_neighbors+1)[1:]\n",
    "            for neighbor in result:\n",
    "                G.add_edge(indx, neighbor)\n",
    "        W = nx.adjacency_matrix(G).todense()\n",
    "        D = np.diag(np.sum(W, axis=1))\n",
    "        return W, D\n",
    "    else:\n",
    "        tree = KDTree(points)\n",
    "        _, indices = tree.query(points, k=k_neighbors+1)  # +1 to include the point itself\n",
    "\n",
    "        n = len(points)\n",
    "        W = np.zeros((n, n))\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in indices[i]:\n",
    "                if i != j:\n",
    "                    W[i, j] = np.exp(-np.linalg.norm(points[i] - points[j])**2)\n",
    "                    W[j, i] = W[i, j]\n",
    "\n",
    "        D = np.diag(np.sum(W, axis=1))\n",
    "        return W, D\n",
    "\n",
    "def find_minimal_connected_graph(points, start_k=None, connected_components=2):\n",
    "    if start_k == None:\n",
    "        start_k = int(np.sqrt(len(points)))\n",
    "    W_start, D_start = construct_graph(points, k_neighbors=start_k)\n",
    "    G_start = nx.from_numpy_array(W_start)\n",
    "    if nx.number_connected_components(G_start) <= connected_components:\n",
    "        W, D = W_start, D_start\n",
    "        for k in reversed(range(1, start_k)):\n",
    "            W_tmp, D_tmp = construct_graph(points, k_neighbors=k)\n",
    "            G = nx.from_numpy_array(W)\n",
    "            if nx.number_connected_components(G) > connected_components:\n",
    "                return W, D, k+1\n",
    "            else:\n",
    "                W, D = W_tmp, D_tmp\n",
    "    if nx.number_connected_components(G_start) > connected_components:\n",
    "        k_max = len(points)//2\n",
    "        W, D = W_start, D_start\n",
    "        for k in range(start_k+1, k_max):\n",
    "            W_tmp, D_tmp = construct_graph(points, k_neighbors=k)\n",
    "            G = nx.from_numpy_array(W)\n",
    "            if nx.number_connected_components(G) <= connected_components:\n",
    "                return W, D, k\n",
    "            else:\n",
    "                W, D = W_tmp, D_tmp\n",
    "    return W_start, D_start, start_k    \n",
    "\n",
    "def compute_heat_kernel(W, D, t, dim):\n",
    "    \"\"\"Compute the heat kernel for a given time t using the adjacency matrix W and degree matrix D.\"\"\"\n",
    "    L = np.linalg.inv(np.sqrt(D)).dot(D - W).dot(np.linalg.inv(np.sqrt(D)))\n",
    "    lambdas, phis = eigsh(L, k=dim, which='SM', tol=1e-5)  # Compute the 10 smallest eigenvalues and eigenvectors\n",
    "\n",
    "    K_t = np.zeros_like(W, dtype=np.float64)\n",
    "    for i in range(len(lambdas)):\n",
    "        K_t += np.exp(-lambdas[i].real * t) * np.outer(phis[:, i].real, phis[:, i].real)\n",
    "\n",
    "    return K_t\n",
    "\n",
    "def sample_using_heat_kernel(points, K_t, num_samples=10):\n",
    "    \"\"\"Sample new points using the heat kernel.\"\"\"\n",
    "    n = len(points)\n",
    "    new_points = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # Pick a random starting point\n",
    "        i = np.random.randint(n)\n",
    "        \n",
    "        # Sample a new point based on the heat distribution from point i\n",
    "        j = np.random.choice(n, p=K_t[i]**2 / np.sum(K_t[i]**2))\n",
    "        \n",
    "        # Take the midpoint between the two points as the new point\n",
    "        new_points.append((points[i] + points[j]) / 2)\n",
    "\n",
    "    return np.array(new_points)\n",
    "\n",
    "def sample_points_heat_kernel(points, num_samples=10, t=0.1, connect_components=1):\n",
    "    \"\"\"Sample new points using the heat kernel.\"\"\"\n",
    "    flag = len(points.shape) > 2\n",
    "    W, D, k = find_minimal_connected_graph(points, connected_components=connect_components, start_k=None)\n",
    "    print(f\"Using {k} nearest neighbors\")\n",
    "    if flag:\n",
    "        re_points = points.reshape(points.shape[0], np.prod(points.shape[1:]))\n",
    "    K_t = compute_heat_kernel(W, D, t, dim=re_points.shape[1])\n",
    "    new_points = sample_using_heat_kernel(re_points, K_t, num_samples=num_samples)\n",
    "    if flag:\n",
    "        new_points = new_points.reshape(num_samples, 1, points.shape[-2], points.shape[-1])\n",
    "    return new_points\n",
    "\n",
    "def uniform_sample(n_samples, dataset):\n",
    "    max_ = np.max(dataset, axis=0) + np.random.uniform(0, 0.1, size=dataset.shape[1])\n",
    "    min_ = np.min(dataset, axis=0) + np.random.uniform(0, 0.1, size=dataset.shape[1])\n",
    "    return np.random.uniform(min_, max_, size=(n_samples, dataset.shape[1]))\n",
    "\n",
    "\n",
    "def batch_form_pullback(start, end, jacobian, form):\n",
    "    jacobian_batch = jacobian[start:end]\n",
    "    form_batch = form[start:end]\n",
    "    return np.einsum('lai,lbj,lck,labc->lijk', jacobian_batch, jacobian_batch, jacobian_batch, form_batch)\n",
    "\n",
    "def compute_jacobian_layer(model, X, layer_indx):\n",
    "    dim_in = model.layers[layer_indx].in_features\n",
    "    dim_out = model.layers[layer_indx].out_features\n",
    "    if dim_out >= dim_in:\n",
    "        jacobian = vmap(jacfwd(model.layers[layer_indx].forward))(X)\n",
    "    else:\n",
    "        jacobian = vmap(jacrev(model.layers[layer_indx].forward))(X)\n",
    "    return jacobian\n",
    "\n",
    "def compute_jacobian_multi_layer(layer_func, X, dim_in, dim_out):\n",
    "    if dim_out >= dim_in:\n",
    "        jacobian = vmap(jacfwd(layer_func))(X)\n",
    "    else:\n",
    "        jacobian = vmap(jacrev(layer_func))(X)\n",
    "    return jacobian\n",
    "\n",
    "\n",
    "def pullback_metric(model, activations, N=50, wrt=\"output_wise\", method=\"lattice\", normalised=False, sigma=0.05):\n",
    "    print('pull 1')\n",
    "    activations_np = [activation.detach().numpy() for activation in activations]\n",
    "    N_layers = len(activations_np)\n",
    "    manifold = LocalDiagPCA(activations_np[-1], sigma=sigma, rho=1e-5)\n",
    "    \n",
    "    if method == \"lattice\":\n",
    "        xy_grid = generate_lattice(activations_np[-1], N)\n",
    "\n",
    "    elif method == \"manifold\":\n",
    "        manifold_2 = LocalDiagPCA(activations_np[0], sigma=sigma, rho=1e-5)\n",
    "        xy_grid = generate_manifold_sample(manifold_2, activations_np[0], N=N**2)\n",
    "        del manifold_2\n",
    "        surface_tensor = torch.from_numpy(xy_grid.reshape(activations_np[0].shape[-2])).float()\n",
    "        model.forward(surface_tensor, save_activations=True)\n",
    "        surfaces = model.get_activations() \n",
    "        _xy_grids = [surface.detach().numpy() for surface in surfaces]\n",
    "        xy_grid = _xy_grids[-1]\n",
    "    elif method == \"heat\":\n",
    "        xy_grid = sample_points_heat_kernel(activations_np[0], num_samples=N**2, connect_components=2, t=1/sigma)\n",
    "        print('Sampled points')\n",
    "        print(xy_grid.shape)\n",
    "        surface_tensor = torch.from_numpy(xy_grid).float()\n",
    "        model.forward(surface_tensor, save_activations=True)\n",
    "        surfaces = model.get_activations()\n",
    "        print('Forwarded surface')\n",
    "        print([s.shape for s in surfaces])\n",
    "        _xy_grids = [surface.detach().numpy() for surface in surfaces]\n",
    "        xy_grid = _xy_grids[-1]\n",
    "    else:\n",
    "        raise ValueError(\"method must be either 'lattice' or 'manifold'\")\n",
    "    g_ = manifold.metric_tensor(xy_grid.transpose(), nargout=1)\n",
    "\n",
    "\n",
    "    g = [0 for _ in activations_np]\n",
    "    \n",
    "    g[-1] = g_\n",
    "\n",
    "    save_grids = [0 for _ in activations_np]\n",
    "    save_grids[-1] = xy_grid\n",
    "    print('checkpoint pre-pullback')\n",
    "\n",
    "    for indx in tqdm(reversed(range(0, N_layers-1))):\n",
    "        dim_in = model.layers[indx].in_features\n",
    "        dim_out = model.layers[indx].out_features\n",
    "        if method == \"manifold\" or method == \"heat\":\n",
    "            xy_grid = _xy_grids[indx]\n",
    "            xy_grid_tensor = torch.from_numpy(xy_grid).float()\n",
    "\n",
    "        elif method == \"lattice\":\n",
    "            xy_grid = generate_lattice(activations_np[indx], N)\n",
    "            xy_grid_tensor = torch.from_numpy(xy_grid).float()\n",
    "\n",
    "        if wrt == \"layer_wise\":\n",
    "            jacobian = compute_jacobian_multi_layer(model.layers[indx], xy_grid_tensor, dim_in, dim_out)\n",
    "            ref = indx + 1\n",
    "\n",
    "        elif wrt == \"output_wise\":\n",
    "            def forward_layers(x):\n",
    "                return model.forward_layers(x, indx)\n",
    "            dim_out = model.layers[-1].out_features\n",
    "\n",
    "            jacobian = compute_jacobian_multi_layer(forward_layers, xy_grid_tensor, dim_in, dim_out)\n",
    "            ref = -1\n",
    "        print('comp jac')\n",
    "        jacobian = jacobian.detach().numpy()\n",
    "        g_pullback = np.einsum('lai,lbj,lab->lij', jacobian, jacobian, g[ref])\n",
    "        print('pulled back')\n",
    "        if normalised:\n",
    "            g_pullback = z_normalise(g_pullback)\n",
    "        g[indx] = g_pullback\n",
    "        save_grids[indx] = xy_grid\n",
    "\n",
    "    return g, save_grids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.data.mnist import MNISTDataset\n",
    "from models.supervised.cnn.model import CNN\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "dataset = MNISTDataset(train=True, root=\"../../../data\")\n",
    "batch_size = 128\n",
    "N_batches = batch_size//8\n",
    "subset_mnist = np.random.randint(0, len(dataset), batch_size**2//8)\n",
    "random_subset = Subset(dataset, subset_mnist)\n",
    "\n",
    "val_data = DataLoader(random_subset, batch_size=batch_size, shuffle=False)\n",
    "mode=\"moon\"\n",
    "model_name = \"cnn\"\n",
    "size = \"vanilla\"\n",
    "\n",
    "cnn_layers = [(1, 16, 3, 1), (16, 32, 3, 1)]\n",
    "fc_layers = [(32 * 24 * 24, 128, nn.ReLU()), (128, 64, nn.ReLU())]\n",
    "output_dim = 10\n",
    "\n",
    "model = CNN(cnn_layers, fc_layers, output_dim)\n",
    "models_path = f\"../../../models/supervised/{model_name}/saved_models\"\n",
    "\n",
    "res_q_25, res_med, res_q_75 = [], [], []\n",
    "\n",
    "\n",
    "tmp = os.listdir(f\"{models_path}/{size}\")\n",
    "\n",
    "epochs = []\n",
    "for i in tmp:\n",
    "    if i[-3:] == \"pth\":\n",
    "        epochs.append(int(i.split('_')[1].split('.')[0]))\n",
    "epochs = sorted(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint 1\n",
      "Gone forward\n",
      "pull 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxpowers/miniconda3/envs/DL/lib/python3.10/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:1592: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
      "  warnings.warn(\"k >= N for N * N square matrix. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 11 nearest neighbors\n",
      "Sampled points\n",
      "(256, 1, 28, 28)\n",
      "Forwarded surface\n",
      "[torch.Size([256, 1, 28, 28]), torch.Size([256, 16, 26, 26]), torch.Size([256, 32, 24, 24]), torch.Size([256, 18432]), torch.Size([256, 128]), torch.Size([256, 64])]\n",
      "checkpoint pre-pullback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp jac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulled back\n",
      "comp jac\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for epoch in tqdm([1]):\n",
    "\tmodel.load_state_dict(torch.load(f'{models_path}/{size}/model_{epoch}.pth'))\n",
    "\tmodel.eval()\n",
    "\tsave_path = f\"figures/{model_name}/{size}/{epoch}/\"\n",
    "\tq_25, med, q_75 = [], [], []\n",
    "\tfor X, y in val_data:\n",
    "\t\tq_25_tmp, med_tmp, q_75_tmp = eigenvalue_results_large(X.detach().numpy(), model, N=int(np.sqrt(len(X)*2)), wrt=\"output_wise\", sigma=0.05, precision=7, save_path=save_path, sampling=\"heat\")\n",
    "\t\tq_25 += q_25_tmp\n",
    "\t\tmed += med_tmp\n",
    "\t\tq_75 += q_75_tmp\n",
    "\n",
    "\tres_q_25.append(q_25)\n",
    "\tres_med.append(med)\n",
    "\tres_q_75.append(q_75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
